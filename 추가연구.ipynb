{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMTdeqMxhVH13aHgSHLgkLe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["neuron에서 작업한 gridsearch 내용입니다."],"metadata":{"id":"hXt6cT8E2zrm"}},{"cell_type":"markdown","source":["### GridSearchCV로 최적 파라미터 찾기"],"metadata":{"id":"soAs1M3G3aYi"}},{"cell_type":"markdown","source":["RandomForest"],"metadata":{"id":"TzSQW_HT4Adf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AA8IvqhV2okY"},"outputs":[],"source":["# 작업 경로 설정\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","\n","# 데이터 로드\n","df = pd.read_csv('train_N.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","\n","from sklearn.model_selection import GridSearchCV\n","import time\n","\n","\n","\n","from sklearn.ensemble import RandomForestRegressor\n","start=time.time()\n","rf = RandomForestRegressor(random_state=1217)\n","\n","# 탐색할 하이퍼파라미터 값 지정\n","params = {'n_estimators' : [200, 300, 500, 700, 900],\n","          'max_features': [5, 6, 8],\n","          'max_depth': [10, 20, 30, 50, 70],\n","          'min_samples_leaf' : [1, 3, 5]}\n","\n","# GridSearchCV를 이용해 최적의 하이퍼파라미터 탐색\n","grid_cv = GridSearchCV(rf, param_grid = params, cv = 3, n_jobs=-1)\n","grid_cv.fit(X_train, y_train)\n","\n","# 최적의 하이퍼파라미터 값과 성능 출력\n","print('################### RF ###########')\n","print('최적 하이퍼파라미터: ', grid_cv.best_params_)\n","print('최적 하이퍼파라미터의 성능(RMSE): {:.4f}'.format(\n","    np.sqrt(grid_cv.best_score_)))\n","\n","end=time.time()\n","print(end-start)\n","\n","print('--------------------------------------------')"]},{"cell_type":"markdown","source":["AdaBoost"],"metadata":{"id":"RA2Q7cQc407z"}},{"cell_type":"code","source":["# 작업 경로 설정\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","\n","# 데이터 로드\n","df = pd.read_csv('train_N.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","\n","from sklearn.model_selection import GridSearchCV\n","import time\n","\n","\n","from sklearn.ensemble import AdaBoostRegressor\n","start=time.time()\n","ada = AdaBoostRegressor()\n","\n","# 탐색할 하이퍼파라미터 값 지정\n","params_ada = {'n_estimators' : [50, 100],\n","          'learning_rate':[0.01,0.05,0.1,0.3,1],  \n","          'loss' : ['linear','square','exponential']} \n","\n","# GridSearchCV를 이용해 최적의 하이퍼파라미터 탐색\n","grid_cv = GridSearchCV(ada, param_grid = params_ada, cv = 5, n_jobs=-1)\n","grid_cv.fit(X_train, y_train)\n","\n","# 최적의 하이퍼파라미터 값과 성능 출력\n","print('################### ada ###########')\n","print('최적 하이퍼파라미터: ', grid_cv.best_params_)\n","print('최적 하이퍼파라미터의 성능(RMSE): {:.4f}'.format(\n","    np.sqrt(grid_cv.best_score_)))\n","\n","end=time.time()\n","print(end-start)\n","\n","print('--------------------------------------------')\n"],"metadata":{"id":"nysWtcRm4UNi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LightGBM"],"metadata":{"id":"HzI4JG6O42-p"}},{"cell_type":"code","source":["# 작업 경로 설정\n","import os\n","\n","import pandas as pd\n","import numpy as np\n","\n","# 데이터 로드\n","df = pd.read_csv('train_N.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","\n","from sklearn.model_selection import GridSearchCV\n","import time\n","\n","\n","import lightgbm as lgb\n","lgbm = lgb.LGBMRegressor()\n","\n","start=time.time()\n","\n","# 탐색할 하이퍼파라미터 값 지정\n","\n","\n","params = {\n","    'num_leaves': [7, 14, 21, 28, 31, 50, 70, 91, 126],\n","    'learning_rate': [0.1, 0.03, 0.003],\n","    'max_depth': [-1, 3, 5],\n","    'n_estimators': [50, 100, 200, 500, 700, 900],\n","}\n","\n","\n","\n","# GridSearchCV를 이용해 최적의 하이퍼파라미터 탐색\n","grid = GridSearchCV(lgbm, param_grid = params, cv = 5, n_jobs=-1)\n","grid.fit(X_train, y_train)\n","\n","# 최적의 하이퍼파라미터 값과 성능 출력\n","print('################### LGBM ###########') \n","print('최적 하이퍼파라미터: ', grid.best_params_) \n","print('최적 하이퍼파라미터의 성능(RMSE): {:.4f}'.format(\n","    np.sqrt(grid.best_score_)))\n","\n","end=time.time()\n","print(end-start)\n","\n","print('--------------------------------------------')\n"],"metadata":{"id":"HXV6WAzp5DvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 유가 Data Scaling"],"metadata":{"id":"WzaPR8C05cNp"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","import scipy as sp\n","#import scipy.stats\n","from sklearn.preprocessing import MinMaxScaler\n","\n","oil = pd.read_csv('oil.csv')\n","\n","oil = oil.drop(['date'], axis='columns')\n","#oil = oil.rename(columns={'date ': 'date'})\n","oil = pd.DataFrame(oil)\n","#oil['target'] = pd.Series(oil.target)\n","\n","# 원래 데이터들의 평균과 분산 확인 \n","print('feature 들의 평균 값')\n","print(oil.mean())\n","print('\\nfeature 들의 분산 값')\n","print(oil.var())\n","\n","# StandardScaler객체 생성\n","scaler = StandardScaler()\n","# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  \n","scaler.fit(oil)\n","oil_scaled = scaler.transform(oil)\n","\n","print(\"\\n=========== 표준화 ==============\\n\")\n","\n","#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n","oil_df_scaled = pd.DataFrame(data=oil_scaled)\n","print('feature 들의 평균 값')\n","print(oil_df_scaled.mean())\n","print('\\nfeature 들의 분산 값')\n","print(oil_df_scaled.var())\n","\n","# check Z score \n","df_Zscore = pd.DataFrame()\n","outlier_dict = {}\n","outlier_idx_list = []\n","\n","for one_col in oil_df_scaled.columns:\n","    print(\"Check\",one_col)\n","    df_Zscore[f'{one_col}_Zscore'] = sp.stats.zscore(oil_df_scaled[one_col])\n","    outlier_dict[one_col] = df_Zscore[f'{one_col}_Zscore'][(df_Zscore[f'{one_col}_Zscore']>2)|(df_Zscore[f'{one_col}_Zscore']<-2)]\n","    outlier_idx_list.append(list(outlier_dict[one_col].index))\n","    if len(outlier_dict[one_col]):\n","        print(one_col,'Has outliers\\n', outlier_dict[one_col])\n","    else:\n","        print(one_col,\"Has Not outlier\")\n","    print()\n","\n","\n","print(\"Before\", oil_df_scaled.shape)\n","all_outlier_idx = sum(outlier_idx_list,[])\n","oil_df_scaled = oil_df_scaled.drop(all_outlier_idx)\n","print(\"After (drop outlier)\", oil_df_scaled.shape)\n","\n","# MinMaxScaler객체 생성\n","scaler = MinMaxScaler()\n","# MinMaxScaler 로 데이터 셋 변환. fit() 과 transform() 호출.  \n","scaler.fit(oil)\n","oil_scaled = scaler.transform(oil)\n","\n","# transform()시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n","oil_df_scaled = pd.DataFrame(data=oil_scaled)\n","print('feature들의 최소 값')\n","print(oil_df_scaled.min())\n","print('\\nfeature들의 최대 값')\n","print(oil_df_scaled.max())\n"],"metadata":{"id":"3TvIACKK5lUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 교차검증"],"metadata":{"id":"yqj3qOdp7Ysp"}},{"cell_type":"markdown","source":["RandomForest"],"metadata":{"id":"OgszKDwQ7vy3"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","rf_model = RandomForestRegressor(max_depth = 30, max_features = 6, min_samples_leaf = 3, n_estimators = 500)\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# cv=5인 교차 검증\n","scores = cross_val_score(rf_model, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","# 성능 확인\n","print('################ rf ##############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))\n"],"metadata":{"id":"6aO3hc5B7r2g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AdaBoost"],"metadata":{"id":"1lg61n587yak"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import AdaBoostRegressor\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","ada_model = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50)\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X = df.drop([ '18~20_ride'], axis='columns')\n","y = df['18~20_ride']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# cv=5인 교차 검증\n","scores = cross_val_score(ada_model, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","# 성능 확인\n","print('################ ada ##############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))\n"],"metadata":{"id":"zzfy9wWv70NQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LightGBM"],"metadata":{"id":"SlrTCtzd74Pf"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","import lightgbm as lgb\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","lgbm_model = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50)\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","# cv=5인 교차 검증\n","scores = cross_val_score(lgbm_model, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","# 성능 확인\n","print('################ lgbm ##############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))\n"],"metadata":{"id":"EgmNh3N875Xv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 모델 앙상블"],"metadata":{"id":"SoqbnJCw6kk6"}},{"cell_type":"markdown","source":["Voting"],"metadata":{"id":"MWkv37uj6oMK"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","#import lightgbm as lgb\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.ensemble import VotingRegressor\n","#from sklearn.svm import SVR\n","import numpy as np\n","import pandas as pd\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X = df.drop([ '18~20_ride'], axis='columns')\n","y = df['18~20_ride']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Training classifiers\n","reg1 = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50)\n","reg2 = RandomForestRegressor(max_depth = 30, max_features = 6, min_samples_leaf = 3, n_estimators = 500)\n","#reg3 = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50)\n","#reg4 = SVR()\n","ereg = VotingRegressor(estimators=[('ada', reg1), ('rf', reg2)])\n","\n","# cv=5인 교차 검증\n","scores = cross_val_score(ereg, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","print('###############ada+rf#############')\n","print('###############cross#############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))"],"metadata":{"id":"yovTP75D6jpK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Bagging"],"metadata":{"id":"g-0I49s66x4s"}},{"cell_type":"code","source":["#import lightgbm as lgb\n","#from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.model_selection import train_test_split\n","#from sklearn.svm import SVR\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.model_selection import cross_val_score\n","import pandas as pd\n","import numpy as np\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X = df.drop([ '18~20_ride'], axis='columns')\n","y = df['18~20_ride']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","#reg1 = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50)\n","#reg2 = RandomForestRegressor(max_depth = 30, max_features = 6, min_samples_leaf = 3, n_estimators = 500)\n","#reg3 = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50)\n","#reg4 = SVR()\n","#estimators=[('ada', reg1), ('rf', reg2), ('lgbm', reg3)]\n","\n","ereg = BaggingRegressor(base_estimator = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50))\n","\n","scores = cross_val_score(ereg, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","print('###############rf#############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))\n","\n"],"metadata":{"id":"qZuskXXo60Wl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stacking"],"metadata":{"id":"TWymVgE565mY"}},{"cell_type":"code","source":["import lightgbm as lgb\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.ensemble import AdaBoostRegressor\n","from sklearn.ensemble import StackingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","#from sklearn.svm import SVR\n","import pandas as pd\n","import numpy as np\n","\n","# 데이터 로드\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X = df.drop([ '18~20_ride'], axis='columns')\n","y = df['18~20_ride']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","reg1 = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50)\n","reg2 = RandomForestRegressor(max_depth = 30, max_features = 6, min_samples_leaf = 3, n_estimators = 500)\n","reg3 = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50)\n","#reg4 = SVR()\n","estimators=[('ada', reg1), ('rf', reg2)]\n","\n","ereg = StackingRegressor(estimators=estimators,final_estimator = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50))\n","\n","scores = cross_val_score(ereg, X_train, y_train, cv=5, n_jobs=-1,\n","                         scoring = 'neg_mean_squared_error')\n","\n","print('###############ada+rf#############')\n","print('cross_val_score \\n{}'.format(np.sqrt(-scores)))\n","print('cross_val_score.mean \\n{:.3f}'.format(np.sqrt(-scores.mean())))\n","\n"],"metadata":{"id":"sDsGLIT46649"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 알고리즘 간 상관관계"],"metadata":{"id":"ImOXFPSv7IHq"}},{"cell_type":"markdown","source":["train.csv train_oil.csv train_weather.csv train_ow.csv 파일을 이용해 RandomForest, AdaBoost, LightGBM 간 상관관계를 구한다."],"metadata":{"id":"agJPdEFj8oP7"}},{"cell_type":"markdown","source":["RandomForest를 이용해 18~20시 버스 승차인원의 예측값을 생성합니다."],"metadata":{"id":"Nw4Pu5bEAxoe"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","import pandas as pd\n","\n","# 데이터 로드\n","sub = pd.read_csv('submission_sample.csv')\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","# 모델\n","model = RandomForestRegressor(max_depth = 30, max_features = 6, min_samples_leaf = 3, n_estimators = 500) \n","\n","model.fit(X_train,y_train)\n","\n","# 예측값 생성\n","pred = model.predict(X_train)\n","sub['18~20_ride'] = pred\n","\n","# csv 파일 저장\n","sub.to_csv('model_ow2_df.csv')\n"],"metadata":{"id":"taGpJkC39MC7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["AdaBoost를 이용해 18~20시 버스 승차인원의 예측값을 생성합니다."],"metadata":{"id":"-u4a78juA58z"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostRegressor\n","import pandas as pd\n","\n","# 데이터 로드\n","sub = pd.read_csv('submission_sample.csv')\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","# 모델\n","model = AdaBoostRegressor(learning_rate = 0.05, loss = 'exponential', n_estimators = 50)\n","\n","model.fit(X_train,y_train)\n","\n","# 예측값 생성\n","pred = model.predict(X_train)\n","sub['18~20_ride'] = pred\n","\n","# csv 파일 저장\n","sub.to_csv('model_ow2_ada.csv')\n"],"metadata":{"id":"LrPi2Dy79O12"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["LightGBM를 이용해 18~20시 버스 승차인원의 예측값을 생성합니다."],"metadata":{"id":"Qq1jZggUA8bL"}},{"cell_type":"code","source":["import lightgbm as lgb\n","import pandas as pd\n","\n","# 데이터 로드\n","sub = pd.read_csv('submission_sample.csv')\n","df = pd.read_csv('train_ow2.csv')\n","\n","# datetime 변수형으로 변환\n","df['date'] = pd.to_datetime(df['date'])\n","# 요일 추출 (0-월요일~6-일요일)\n","df['weekday'] = df['date'].dt.weekday\n","df['in_out'] = df['in_out'].map({'시내':0, '시외':1})\n","\n","df.head()\n","datetime_column = list(df.columns[df.dtypes == 'datetime64[ns]'])\n","# datetime형 변수 제거\n","df = df.drop(datetime_column, axis='columns')\n","df = df.drop([ 'station_name'], axis='columns')\n","\n","# 학습 데이터\n","X_train = df.drop([ '18~20_ride'], axis='columns')\n","y_train = df['18~20_ride']\n","\n","# 모델\n","model = lgb.LGBMRegressor(learning_rate = 0.1, max_depth = -1, n_estimators = 500, num_leaves = 50) \n","\n","model.fit(X_train,y_train)\n","\n","# 예측값 생성\n","pred = model.predict(X_train)\n","sub['18~20_ride'] = pred\n","\n","# csv 파일 저장\n","sub.to_csv('model_ow2_lgbm.csv')\n"],"metadata":{"id":"3ouLlcDI9R38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["같은 csv파일을 이용해서 생성된 모델간의 상관관계를 구한다."],"metadata":{"id":"CJpcwPufA_lm"}},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","model_rf = pd.read_csv('model_w_rf.csv')\n","model_ada = pd.read_csv('model_w_ada.csv')\n","model_lgbm = pd.read_csv('model_w_lgbm.csv')\n","\n","list = [model_rf, model_ada, model_lgbm]\n","\n","# 상관계수 행렬을 저장할 데이터프레임 생성\n","corr_df = pd.DataFrame()\n","\n","for i in list:\n","\ttakeoff = i['18~20_ride']\n","\ttakeoff.columns = [str(i)]\n","\tcorr_df = pd.concat([corr_df, takeoff], axis=1)\n","\n","# 상관계수 행렬 출력    \n","result = corr_df.corr()\n","result.to_csv('correlation_w.csv')\n"],"metadata":{"id":"n3po5yDE7M9k"},"execution_count":null,"outputs":[]}]}